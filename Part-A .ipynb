{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca3a568-cf84-4a07-a0af-b69f5665302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d5b475-45ab-479b-8fe1-81376f1e6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_dataset.csv' with the actual path to your CSV file.\n",
    "csv_file = 'all_stocks_5yr.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(\"CSV data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125646-c529-45f8-8a8c-1f93fe174a0b",
   "metadata": {},
   "source": [
    "## Convert and Save as Parquet (with Compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae7ecec-22fe-46c0-8a18-2f2be8f73e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been successfully saved to a Parquet file with Snappy compression.\n"
     ]
    }
   ],
   "source": [
    "parquet_file = 'your_dataset_snappy.parquet'\n",
    "df.to_parquet(parquet_file, compression='snappy', index=False)\n",
    "print(\"The data has been successfully saved to a Parquet file with Snappy compression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f79084c-800e-4aa0-910a-4e956762f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_duplicate(df, rows_to_add):\n",
    "    \"\"\"Append 'rows_to_add' rows to the DataFrame by duplicating the first rows.\"\"\"\n",
    "    new_rows = df.head(rows_to_add)\n",
    "    return pd.concat([df, new_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c10768-2319-4324-9738-48fd1515254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] \n",
    "scales = [1, 10, 100] \n",
    "scale_labels = [\"1x\", \"10x\", \"100x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dd3ced-2e29-4369-bead-aa996797ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_scale(df, factor):\n",
    "    \"\"\"Duplicate the DataFrame 'factor' times.\"\"\"\n",
    "    return pd.concat([df] * factor, ignore_index=True)\n",
    "\n",
    "# Now the rest of your code:\n",
    "for factor, label in zip(scales, scale_labels):\n",
    "    if factor == 1:\n",
    "        df_scaled = df\n",
    "    else:\n",
    "        df_scaled = simulate_scale(df, factor)\n",
    "    \n",
    "    # Generate temporary filenames for the scaled CSV and Parquet files\n",
    "    csv_file = f'scaled_dataset_{label}.csv'\n",
    "    parquet_file = f'scaled_dataset_{label}_snappy.parquet'\n",
    "    \n",
    "    # --- Measure CSV Write Performance ---\n",
    "    start_time = time.time()\n",
    "    df_scaled.to_csv(csv_file, index=False)\n",
    "    csv_write_duration = time.time() - start_time\n",
    "    \n",
    "    # --- Measure CSV Read Performance ---\n",
    "    start_time = time.time()\n",
    "    _ = pd.read_csv(csv_file)\n",
    "    csv_read_duration = time.time() - start_time\n",
    "    \n",
    "    # --- Measure Parquet Write Performance (using Snappy compression) ---\n",
    "    start_time = time.time()\n",
    "    df_scaled.to_parquet(parquet_file, compression='snappy', index=False)\n",
    "    parquet_write_duration = time.time() - start_time\n",
    "    \n",
    "    # --- Measure Parquet Read Performance ---\n",
    "    start_time = time.time()\n",
    "    _ = pd.read_parquet(parquet_file)\n",
    "    parquet_read_duration = time.time() - start_time\n",
    " \n",
    "    # --- Calculate File Sizes (in MB) ---\n",
    "    csv_file_size = os.path.getsize(csv_file) / (1024 * 1024)\n",
    "    parquet_file_size = os.path.getsize(parquet_file) / (1024 * 1024)\n",
    "\n",
    "    # Record the benchmark results in a dictionary\n",
    "    results.append({\n",
    "        \"Scale\": label,\n",
    "        \"CSV Write Duration (s)\": round(csv_write_duration, 2),\n",
    "        \"CSV Read Duration (s)\": round(csv_read_duration, 2),\n",
    "        \"Parquet Write Duration (s)\": round(parquet_write_duration, 2),\n",
    "        \"Parquet Read Duration (s)\": round(parquet_read_duration, 2),\n",
    "        \"CSV File Size (MB)\": round(csv_file_size, 2),\n",
    "        \"Parquet File Size (MB)\": round(parquet_file_size, 2)\n",
    "    })\n",
    "\n",
    "# Clean up the temporary files after benchmarking\n",
    "os.remove(csv_file)\n",
    "os.remove(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec14624-1686-4e7f-aece-546c0829db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results Summary:\n",
      "  Scale  CSV Write Duration (s)  CSV Read Duration (s)  \\\n",
      "0    1x                    1.99                   0.32   \n",
      "1   10x                   27.53                   4.20   \n",
      "2  100x                  285.12                  60.58   \n",
      "\n",
      "   Parquet Write Duration (s)  Parquet Read Duration (s)  CSV File Size (MB)  \\\n",
      "0                        0.28                       0.18               28.80   \n",
      "1                        3.22                       0.94              288.01   \n",
      "2                       44.56                      26.72             2880.05   \n",
      "\n",
      "   Parquet File Size (MB)  \n",
      "0                   10.15  \n",
      "1                   95.35  \n",
      "2                  951.67  \n"
     ]
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame and print the summary.\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nBenchmark Results Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca3a58-f58f-4bbd-842c-66c9f912d51d",
   "metadata": {},
   "source": [
    "## Part A Summary:\n",
    "> Faster Read/Write: Parquet significantly outperforms CSV in both read and write times, especially as dataset size increases.\n",
    "\n",
    "> Smaller File Size: Parquet files are compressed and take up ∼35% of the space compared to CSV files at all scales.\n",
    "\n",
    "> Better Scaling: Parquet's efficiency becomes more evident at larger scales (10× and 100×), with faster operations and reduced storage requirements.\n",
    "\n",
    "> Recommendation: For large-scale datasets, Parquet is the better option for analytical workloads due to its speed and compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
